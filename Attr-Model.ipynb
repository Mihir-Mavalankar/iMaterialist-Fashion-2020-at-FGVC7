{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:28:49.136146Z",
     "start_time": "2020-05-22T01:28:45.715973Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torchvision \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:29:08.865146Z",
     "start_time": "2020-05-22T01:28:49.268104Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('data/imaterialist-fashion-2020-fgvc7/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:29:16.126542Z",
     "start_time": "2020-05-22T01:29:09.035027Z"
    }
   },
   "outputs": [],
   "source": [
    "attr = train_df.AttributesIds\n",
    "clss = train_df.ClassId\n",
    "not_exist = attr.isna()\n",
    "non_nan_classes=[]\n",
    "for i in range(len(not_exist)):\n",
    "    if not not_exist[i]:\n",
    "        if clss[i] not in non_nan_classes:\n",
    "            non_nan_classes.append(clss[i])\n",
    "            \n",
    "non_nan_classes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:29:52.962067Z",
     "start_time": "2020-05-22T01:29:52.832109Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df = pd.DataFrame(train_df[train_df.ClassId.isin(non_nan_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:29:53.706033Z",
     "start_time": "2020-05-22T01:29:53.701033Z"
    }
   },
   "outputs": [],
   "source": [
    "class CONFIG():\n",
    "    LABELS_LENGTH = 294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:29:54.413322Z",
     "start_time": "2020-05-22T01:29:54.404316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class AttrModel(nn.Module):\\n    def __init__(self, num_classes=294):\\n        super(AttrModel, self).__init__()\\n        self.num_classes = num_classes\\n        Base_Model = torchvision.models.resnet18(pretrained=True)\\n        \\n        children = list(Base_Model.children())\\n        \\n        self.conv = children[0]\\n        self.batch_norm = children[1]\\n        self.relu = children[2]\\n        self.maxpool = children[3]\\n        self.blocks1 = children[4]\\n        self.blocks2 = children[5]\\n        self.blocks3 = children[6]\\n        self.blocks4 = children[7]\\n        self.adp_avg_pool = children[8]\\n        self.fc = nn.Linear(children[9].in_features, self.num_classes)\\n        \\n        \\n    def forward(self, x):\\n        x = self.conv(x)\\n        x = self.batch_norm(x)\\n        x = self.relu(x)\\n        x = self.maxpool(x)\\n        x = self.blocks1(x)\\n        x = self.blocks2(x)\\n        x = self.blocks3(x)\\n        x = self.blocks4(x)\\n        x = self.adp_avg_pool(x)\\n        x = self.fc(x)\\n        x = torch.nn.functional.sigmoid(x)\\n        return x'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class AttrModel(nn.Module):\n",
    "    def __init__(self, num_classes=294):\n",
    "        super(AttrModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        Base_Model = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        children = list(Base_Model.children())\n",
    "        \n",
    "        self.conv = children[0]\n",
    "        self.batch_norm = children[1]\n",
    "        self.relu = children[2]\n",
    "        self.maxpool = children[3]\n",
    "        self.blocks1 = children[4]\n",
    "        self.blocks2 = children[5]\n",
    "        self.blocks3 = children[6]\n",
    "        self.blocks4 = children[7]\n",
    "        self.adp_avg_pool = children[8]\n",
    "        self.fc = nn.Linear(children[9].in_features, self.num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.blocks1(x)\n",
    "        x = self.blocks2(x)\n",
    "        x = self.blocks3(x)\n",
    "        x = self.blocks4(x)\n",
    "        x = self.adp_avg_pool(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.nn.functional.sigmoid(x)\n",
    "        return x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:29:56.752921Z",
     "start_time": "2020-05-22T01:29:56.703936Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Attr_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, config,  df):\n",
    "        self.IMGS = list(filtered_df.ImageId)\n",
    "        self.config = config\n",
    "        self.df = df\n",
    "        \n",
    "        self.Enc = list(self.df.EncodedPixels)\n",
    "        self.H = list(self.df.Height)\n",
    "        self.W = list(self.df.Width)\n",
    "        self.Attr = list(self.df.AttributesIds)\n",
    "        \n",
    "    def vectorize_labels(self, labels):\n",
    "        vector = np.array([0]*self.config.LABELS_LENGTH)\n",
    "        for idx in labels:\n",
    "            vector[idx]=1\n",
    "        return vector\n",
    "            \n",
    "    def change_label_indices(self, labels):\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i]>234:\n",
    "                labels[i]-=47\n",
    "        return labels\n",
    "    \n",
    "    def make_single_mask(self, encoded_string, height, width):\n",
    "        splitted_string = np.array(list(map(int, encoded_string.split()))).reshape(-1,2)\n",
    "        mask = np.zeros((height*width), dtype=np.uint8)\n",
    "        for start_indice, run_length in splitted_string:\n",
    "            start_indice-=1\n",
    "            mask[start_indice:start_indice+run_length] = 1\n",
    "        return mask.reshape((height, width), order='F')\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = np.array(cv2.cvtColor(cv2.imread('data/imaterialist-fashion-2020-fgvc7/train/' + self.IMGS[idx] + '.jpg'), cv2.COLOR_BGR2RGB)/255, dtype=np.float32)\n",
    "        mask = self.make_single_mask(self.Enc[idx], self.H[idx], self.W[idx])\n",
    "        mask_ = np.stack((mask,mask,mask), axis=-1)\n",
    "        img = img*mask_\n",
    "        img = cv2.resize(img, (1024, 1024), interpolation = cv2.INTER_AREA)\n",
    "        labels = self.Attr[idx]\n",
    "        if not (labels!=labels):        \n",
    "            labels = self.change_label_indices(list(map(int, labels.split(','))))\n",
    "        else:\n",
    "            labels=[]\n",
    "        label_vector = self.vectorize_labels(labels)\n",
    "        \n",
    "        \n",
    "        return np.moveaxis(img, -1, 0), label_vector\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.IMGS)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:29:59.047407Z",
     "start_time": "2020-05-22T01:29:59.032412Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttrModel(nn.Module):\n",
    "    def __init__(self, num_classes=294):\n",
    "        super(AttrModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        Base_Model = torchvision.models.resnet18(pretrained=True, progress=True)\n",
    "        Base_Model.fc = nn.Linear(Base_Model.fc.in_features, num_classes)\n",
    "        self.Model_Backend = Base_Model\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.Model_Backend(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:30:01.400018Z",
     "start_time": "2020-05-22T01:30:01.392018Z"
    }
   },
   "outputs": [],
   "source": [
    "saving_steps = 200\n",
    "epoch=0\n",
    "batch_size=12\n",
    "last = 672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:30:03.231801Z",
     "start_time": "2020-05-22T01:30:02.146322Z"
    }
   },
   "outputs": [],
   "source": [
    "#AttrM = AttrModel()\n",
    "AttrM = torch.load('models/Base.pt')   \n",
    "conf = CONFIG()\n",
    "Attr_dataset = Attr_Dataset(conf, filtered_df)\n",
    "Attr_data_loader = torch.utils.data.DataLoader(Attr_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T01:30:08.391797Z",
     "start_time": "2020-05-22T01:30:04.406213Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "params = [p for p in AttrM.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,momentum=0.9, weight_decay=0.0005)\n",
    "DEVICE = torch.device('cuda:1')\n",
    "_ = AttrM.to(DEVICE)\n",
    "_ = AttrM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T06:35:54.522352Z",
     "start_time": "2020-05-22T01:30:41.384920Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2b6a6204d44df482f0885e843ed470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19013), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  672 is 0.05351759493350983\n",
      "loss at step  673 is 0.05278264358639717\n",
      "loss at step  674 is 0.0606030598282814\n",
      "loss at step  675 is 0.048182904720306396\n",
      "loss at step  676 is 0.06026555597782135\n",
      "loss at step  677 is 0.04453951492905617\n",
      "loss at step  678 is 0.0445040799677372\n",
      "loss at step  679 is 0.0501612164080143\n",
      "loss at step  680 is 0.050962407141923904\n",
      "loss at step  681 is 0.056361038237810135\n",
      "loss at step  682 is 0.05171194300055504\n",
      "loss at step  683 is 0.049835916608572006\n",
      "loss at step  684 is 0.054386772215366364\n",
      "loss at step  685 is 0.05372697114944458\n",
      "loss at step  686 is 0.050018180161714554\n",
      "loss at step  687 is 0.061788804829120636\n",
      "loss at step  688 is 0.04037660360336304\n",
      "loss at step  689 is 0.056798990815877914\n",
      "loss at step  690 is 0.04131759703159332\n",
      "loss at step  691 is 0.05348939076066017\n",
      "loss at step  692 is 0.04956731200218201\n",
      "loss at step  693 is 0.04834343120455742\n",
      "loss at step  694 is 0.04926472529768944\n",
      "loss at step  695 is 0.056187793612480164\n",
      "loss at step  696 is 0.04895782470703125\n",
      "loss at step  697 is 0.04403551295399666\n",
      "loss at step  698 is 0.05854795128107071\n",
      "loss at step  699 is 0.05384967476129532\n",
      "loss at step  700 is 0.04509377107024193\n",
      "loss at step  701 is 0.047353312373161316\n",
      "loss at step  702 is 0.046865325421094894\n",
      "loss at step  703 is 0.059234075248241425\n",
      "loss at step  704 is 0.047682151198387146\n",
      "loss at step  705 is 0.056945424526929855\n",
      "loss at step  706 is 0.04976845160126686\n",
      "loss at step  707 is 0.051027920097112656\n",
      "loss at step  708 is 0.06402398645877838\n",
      "loss at step  709 is 0.0455387718975544\n",
      "loss at step  710 is 0.0444490984082222\n",
      "loss at step  711 is 0.05623789131641388\n",
      "loss at step  712 is 0.05294814705848694\n",
      "loss at step  713 is 0.0477103590965271\n",
      "loss at step  714 is 0.0455436110496521\n",
      "loss at step  715 is 0.0608719140291214\n",
      "loss at step  716 is 0.060250069946050644\n",
      "loss at step  717 is 0.0492415577173233\n",
      "loss at step  718 is 0.04307233542203903\n",
      "loss at step  719 is 0.05063288286328316\n",
      "loss at step  720 is 0.055007439106702805\n",
      "loss at step  721 is 0.03757930174469948\n",
      "loss at step  722 is 0.053496699780225754\n",
      "loss at step  723 is 0.04705014079809189\n",
      "loss at step  724 is 0.045681364834308624\n",
      "loss at step  725 is 0.04478883743286133\n",
      "loss at step  726 is 0.05585574358701706\n",
      "loss at step  727 is 0.05284490808844566\n",
      "loss at step  728 is 0.05258208140730858\n",
      "loss at step  729 is 0.04694662243127823\n",
      "loss at step  730 is 0.05355934053659439\n",
      "loss at step  731 is 0.04794573783874512\n",
      "loss at step  732 is 0.056536756455898285\n",
      "loss at step  733 is 0.05413184314966202\n",
      "loss at step  734 is 0.056192170828580856\n",
      "loss at step  735 is 0.056713469326496124\n",
      "loss at step  736 is 0.045531585812568665\n",
      "loss at step  737 is 0.05336650833487511\n",
      "loss at step  738 is 0.0579574853181839\n",
      "loss at step  739 is 0.04824578016996384\n",
      "loss at step  740 is 0.049953825771808624\n",
      "loss at step  741 is 0.04532461240887642\n",
      "loss at step  742 is 0.046935681253671646\n",
      "loss at step  743 is 0.05481206253170967\n",
      "loss at step  744 is 0.03601500764489174\n",
      "loss at step  745 is 0.047795627266168594\n",
      "loss at step  746 is 0.042912594974040985\n",
      "loss at step  747 is 0.05109613016247749\n",
      "loss at step  748 is 0.04278741031885147\n",
      "loss at step  749 is 0.0577201247215271\n",
      "loss at step  750 is 0.04644908383488655\n",
      "loss at step  751 is 0.06590835750102997\n",
      "loss at step  752 is 0.0415908508002758\n",
      "loss at step  753 is 0.05809413269162178\n",
      "loss at step  754 is 0.04538239166140556\n",
      "loss at step  755 is 0.04416606202721596\n",
      "loss at step  756 is 0.056664180010557175\n",
      "loss at step  757 is 0.05563407763838768\n",
      "loss at step  758 is 0.048587873578071594\n",
      "loss at step  759 is 0.046875961124897\n",
      "loss at step  760 is 0.05004410445690155\n",
      "loss at step  761 is 0.05231286957859993\n",
      "loss at step  762 is 0.04856843128800392\n",
      "loss at step  763 is 0.04693906009197235\n",
      "loss at step  764 is 0.04854976385831833\n",
      "loss at step  765 is 0.0318155474960804\n",
      "loss at step  766 is 0.0564655177295208\n",
      "loss at step  767 is 0.048748258501291275\n",
      "loss at step  768 is 0.05277945101261139\n",
      "loss at step  769 is 0.051032327115535736\n",
      "loss at step  770 is 0.04557914659380913\n",
      "loss at step  771 is 0.05924557149410248\n",
      "loss at step  772 is 0.04869447648525238\n",
      "loss at step  773 is 0.05502789840102196\n",
      "loss at step  774 is 0.05125368386507034\n",
      "loss at step  775 is 0.04380391165614128\n",
      "loss at step  776 is 0.052128106355667114\n",
      "loss at step  777 is 0.05470810830593109\n",
      "loss at step  778 is 0.047983165830373764\n",
      "loss at step  779 is 0.034704867750406265\n",
      "loss at step  780 is 0.04794996976852417\n",
      "loss at step  781 is 0.04837654531002045\n",
      "loss at step  782 is 0.04896404221653938\n",
      "loss at step  783 is 0.04748593270778656\n",
      "loss at step  784 is 0.04928575083613396\n",
      "loss at step  785 is 0.05107869207859039\n",
      "loss at step  786 is 0.04859822243452072\n",
      "loss at step  787 is 0.04381314665079117\n",
      "loss at step  788 is 0.05568286031484604\n",
      "loss at step  789 is 0.04489409178495407\n",
      "loss at step  790 is 0.041256725788116455\n",
      "loss at step  791 is 0.04214254394173622\n",
      "loss at step  792 is 0.039016637951135635\n",
      "loss at step  793 is 0.05102435499429703\n",
      "loss at step  794 is 0.038815688341856\n",
      "loss at step  795 is 0.044510163366794586\n",
      "loss at step  796 is 0.04140380397439003\n",
      "loss at step  797 is 0.05082898214459419\n",
      "loss at step  798 is 0.05222177132964134\n",
      "loss at step  799 is 0.05099548026919365\n",
      "loss at step  800 is 0.045534200966358185\n",
      "================================  Model Saved  ================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type AttrModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  801 is 0.04177973419427872\n",
      "loss at step  802 is 0.05586016923189163\n",
      "loss at step  803 is 0.046533066779375076\n",
      "loss at step  804 is 0.04777514562010765\n",
      "loss at step  805 is 0.04107840731739998\n",
      "loss at step  806 is 0.049701716750860214\n",
      "loss at step  807 is 0.045169346034526825\n",
      "loss at step  808 is 0.05805554240942001\n",
      "loss at step  809 is 0.04891274496912956\n",
      "loss at step  810 is 0.049430664628744125\n",
      "loss at step  811 is 0.05674364045262337\n",
      "loss at step  812 is 0.046965692192316055\n",
      "loss at step  813 is 0.048289086669683456\n",
      "loss at step  814 is 0.055626947432756424\n",
      "loss at step  815 is 0.0428868867456913\n",
      "loss at step  816 is 0.054217997938394547\n",
      "loss at step  817 is 0.04882197082042694\n",
      "loss at step  818 is 0.04798629879951477\n",
      "loss at step  819 is 0.04907384142279625\n",
      "loss at step  820 is 0.048190098255872726\n",
      "loss at step  821 is 0.051931459456682205\n",
      "loss at step  822 is 0.04793308675289154\n",
      "loss at step  823 is 0.04350828379392624\n",
      "loss at step  824 is 0.048675935715436935\n",
      "loss at step  825 is 0.04227732867002487\n",
      "loss at step  826 is 0.05216308683156967\n",
      "loss at step  827 is 0.044706135988235474\n",
      "loss at step  828 is 0.04607035964727402\n",
      "loss at step  829 is 0.0429992750287056\n",
      "loss at step  830 is 0.05097288638353348\n",
      "loss at step  831 is 0.049959201365709305\n",
      "loss at step  832 is 0.05204387381672859\n",
      "loss at step  833 is 0.042700111865997314\n",
      "loss at step  834 is 0.029074903577566147\n",
      "loss at step  835 is 0.04119926691055298\n",
      "loss at step  836 is 0.05094849318265915\n",
      "loss at step  837 is 0.05843855068087578\n",
      "loss at step  838 is 0.04237597435712814\n",
      "loss at step  839 is 0.04112095385789871\n",
      "loss at step  840 is 0.04490413889288902\n",
      "loss at step  841 is 0.0499236024916172\n",
      "loss at step  842 is 0.04836798459291458\n",
      "loss at step  843 is 0.046748943626880646\n",
      "loss at step  844 is 0.04179214686155319\n",
      "loss at step  845 is 0.04449302330613136\n",
      "loss at step  846 is 0.04306181147694588\n",
      "loss at step  847 is 0.04743650183081627\n",
      "loss at step  848 is 0.04345580190420151\n",
      "loss at step  849 is 0.0516757033765316\n",
      "loss at step  850 is 0.04610443115234375\n",
      "loss at step  851 is 0.04025610163807869\n",
      "loss at step  852 is 0.04357416555285454\n",
      "loss at step  853 is 0.043780796229839325\n",
      "loss at step  854 is 0.04760046675801277\n",
      "loss at step  855 is 0.0494086854159832\n",
      "loss at step  856 is 0.04680579900741577\n",
      "loss at step  857 is 0.05530258268117905\n",
      "loss at step  858 is 0.040198978036642075\n",
      "loss at step  859 is 0.03903532028198242\n",
      "loss at step  860 is 0.05842112377285957\n",
      "loss at step  861 is 0.044494882225990295\n",
      "loss at step  862 is 0.04765912890434265\n",
      "loss at step  863 is 0.04430244490504265\n",
      "loss at step  864 is 0.06154804304242134\n",
      "loss at step  865 is 0.06021048501133919\n",
      "loss at step  866 is 0.03778400644659996\n",
      "loss at step  867 is 0.050505589693784714\n",
      "loss at step  868 is 0.046788282692432404\n",
      "loss at step  869 is 0.041123148053884506\n",
      "loss at step  870 is 0.05279185250401497\n",
      "loss at step  871 is 0.0478006973862648\n",
      "loss at step  872 is 0.0505177341401577\n",
      "loss at step  873 is 0.05697343870997429\n",
      "loss at step  874 is 0.04512431100010872\n",
      "loss at step  875 is 0.04462255537509918\n",
      "loss at step  876 is 0.04011864587664604\n",
      "loss at step  877 is 0.04338296875357628\n",
      "loss at step  878 is 0.04779014736413956\n",
      "loss at step  879 is 0.048951294273138046\n",
      "loss at step  880 is 0.044978391379117966\n",
      "loss at step  881 is 0.053952835500240326\n",
      "loss at step  882 is 0.03867116943001747\n",
      "loss at step  883 is 0.038713231682777405\n",
      "loss at step  884 is 0.04750370606780052\n",
      "loss at step  885 is 0.04311145097017288\n",
      "loss at step  886 is 0.033474355936050415\n",
      "loss at step  887 is 0.03844184800982475\n",
      "loss at step  888 is 0.04677284136414528\n",
      "loss at step  889 is 0.05301384627819061\n",
      "loss at step  890 is 0.04499555379152298\n",
      "loss at step  891 is 0.04897123575210571\n",
      "loss at step  892 is 0.052421070635318756\n",
      "loss at step  893 is 0.03473186492919922\n",
      "loss at step  894 is 0.04384579509496689\n",
      "loss at step  895 is 0.06557083874940872\n",
      "loss at step  896 is 0.04416739195585251\n",
      "loss at step  897 is 0.04664772376418114\n",
      "loss at step  898 is 0.05309902876615524\n",
      "loss at step  899 is 0.04573651775717735\n",
      "loss at step  900 is 0.04859919100999832\n",
      "loss at step  901 is 0.044090017676353455\n",
      "loss at step  902 is 0.03450140729546547\n",
      "loss at step  903 is 0.04044487327337265\n",
      "loss at step  904 is 0.04999620094895363\n",
      "loss at step  905 is 0.04246259480714798\n",
      "loss at step  906 is 0.046390291303396225\n",
      "loss at step  907 is 0.047499630600214005\n",
      "loss at step  908 is 0.03450164571404457\n",
      "loss at step  909 is 0.03680478036403656\n",
      "loss at step  910 is 0.04838939383625984\n",
      "loss at step  911 is 0.044957470148801804\n",
      "loss at step  912 is 0.051996659487485886\n",
      "loss at step  913 is 0.03550567477941513\n",
      "loss at step  914 is 0.05389136075973511\n",
      "loss at step  915 is 0.05248817801475525\n",
      "loss at step  916 is 0.038368068635463715\n",
      "loss at step  917 is 0.0474480576813221\n",
      "loss at step  918 is 0.039549246430397034\n",
      "loss at step  919 is 0.05567482113838196\n",
      "loss at step  920 is 0.04135189950466156\n",
      "loss at step  921 is 0.047433603554964066\n",
      "loss at step  922 is 0.04030999168753624\n",
      "loss at step  923 is 0.04979069158434868\n",
      "loss at step  924 is 0.04097099229693413\n",
      "loss at step  925 is 0.05412318557500839\n",
      "loss at step  926 is 0.047549769282341\n",
      "loss at step  927 is 0.054788049310445786\n",
      "loss at step  928 is 0.04323481768369675\n",
      "loss at step  929 is 0.03990767523646355\n",
      "loss at step  930 is 0.051165688782930374\n",
      "loss at step  931 is 0.04420212656259537\n",
      "loss at step  932 is 0.046515390276908875\n",
      "loss at step  933 is 0.05895671248435974\n",
      "loss at step  934 is 0.039663396775722504\n",
      "loss at step  935 is 0.06292161345481873\n",
      "loss at step  936 is 0.03417377173900604\n",
      "loss at step  937 is 0.06031101197004318\n",
      "loss at step  938 is 0.04991930350661278\n",
      "loss at step  939 is 0.052030809223651886\n",
      "loss at step  940 is 0.05187632516026497\n",
      "loss at step  941 is 0.043907053768634796\n",
      "loss at step  942 is 0.042502887547016144\n",
      "loss at step  943 is 0.0488465242087841\n",
      "loss at step  944 is 0.034384313970804214\n",
      "loss at step  945 is 0.051121339201927185\n",
      "loss at step  946 is 0.044431593269109726\n",
      "loss at step  947 is 0.04945379123091698\n",
      "loss at step  948 is 0.042719338089227676\n",
      "loss at step  949 is 0.04459499567747116\n",
      "loss at step  950 is 0.05463644862174988\n",
      "loss at step  951 is 0.04774783179163933\n",
      "loss at step  952 is 0.04674166068434715\n",
      "loss at step  953 is 0.04448859021067619\n",
      "loss at step  954 is 0.035294901579618454\n",
      "loss at step  955 is 0.035724956542253494\n",
      "loss at step  956 is 0.05584295094013214\n",
      "loss at step  957 is 0.04752953723073006\n",
      "loss at step  958 is 0.03197247162461281\n",
      "loss at step  959 is 0.03469476476311684\n",
      "loss at step  960 is 0.04550738260149956\n",
      "loss at step  961 is 0.04815973341464996\n",
      "loss at step  962 is 0.05289256572723389\n",
      "loss at step  963 is 0.05030680075287819\n",
      "loss at step  964 is 0.055276211351156235\n",
      "loss at step  965 is 0.0504930205643177\n",
      "loss at step  966 is 0.045136794447898865\n",
      "loss at step  967 is 0.04647205397486687\n",
      "loss at step  968 is 0.03702735900878906\n",
      "loss at step  969 is 0.059914425015449524\n",
      "loss at step  970 is 0.0523679293692112\n",
      "loss at step  971 is 0.04749591648578644\n",
      "loss at step  972 is 0.04474874213337898\n",
      "loss at step  973 is 0.050800636410713196\n",
      "loss at step  974 is 0.037203021347522736\n",
      "loss at step  975 is 0.04225347191095352\n",
      "loss at step  976 is 0.05121074616909027\n",
      "loss at step  977 is 0.04359421506524086\n",
      "loss at step  978 is 0.04226496070623398\n",
      "loss at step  979 is 0.04287382587790489\n",
      "loss at step  980 is 0.05504352226853371\n",
      "loss at step  981 is 0.04411739483475685\n",
      "loss at step  982 is 0.04702136293053627\n",
      "loss at step  983 is 0.04470249265432358\n",
      "loss at step  984 is 0.062498848885297775\n",
      "loss at step  985 is 0.04045869782567024\n",
      "loss at step  986 is 0.05217994749546051\n",
      "loss at step  987 is 0.04354211315512657\n",
      "loss at step  988 is 0.043709851801395416\n",
      "loss at step  989 is 0.040605008602142334\n",
      "loss at step  990 is 0.03552265092730522\n",
      "loss at step  991 is 0.04811828210949898\n",
      "loss at step  992 is 0.04201289638876915\n",
      "loss at step  993 is 0.04710933193564415\n",
      "loss at step  994 is 0.05567165091633797\n",
      "loss at step  995 is 0.04671413451433182\n",
      "loss at step  996 is 0.046775542199611664\n",
      "loss at step  997 is 0.04334874823689461\n",
      "loss at step  998 is 0.04029010608792305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  999 is 0.04601070657372475\n",
      "loss at step  1000 is 0.03405667841434479\n",
      "================================  Model Saved  ================================\n",
      "loss at step  1001 is 0.04190324619412422\n",
      "loss at step  1002 is 0.042517293244600296\n",
      "loss at step  1003 is 0.03898550570011139\n",
      "loss at step  1004 is 0.04385773092508316\n",
      "loss at step  1005 is 0.051316164433956146\n",
      "loss at step  1006 is 0.048057921230793\n",
      "loss at step  1007 is 0.03624714910984039\n",
      "loss at step  1008 is 0.047682877629995346\n",
      "loss at step  1009 is 0.03340322524309158\n",
      "loss at step  1010 is 0.029842033982276917\n",
      "loss at step  1011 is 0.043463047593832016\n",
      "loss at step  1012 is 0.050587091594934464\n",
      "loss at step  1013 is 0.0379394069314003\n",
      "loss at step  1014 is 0.056424304842948914\n",
      "loss at step  1015 is 0.047186266630887985\n",
      "loss at step  1016 is 0.046380914747714996\n",
      "loss at step  1017 is 0.05592458322644234\n",
      "loss at step  1018 is 0.040074706077575684\n",
      "loss at step  1019 is 0.04412051662802696\n",
      "loss at step  1020 is 0.0516241155564785\n",
      "loss at step  1021 is 0.032701969146728516\n",
      "loss at step  1022 is 0.03965651988983154\n",
      "loss at step  1023 is 0.0345512218773365\n",
      "loss at step  1024 is 0.04700767993927002\n",
      "loss at step  1025 is 0.04671316221356392\n",
      "loss at step  1026 is 0.04450647532939911\n",
      "loss at step  1027 is 0.04785865172743797\n",
      "loss at step  1028 is 0.055729709565639496\n",
      "loss at step  1029 is 0.0395224466919899\n",
      "loss at step  1030 is 0.041361648589372635\n",
      "loss at step  1031 is 0.05172737315297127\n",
      "loss at step  1032 is 0.03594692796468735\n",
      "loss at step  1033 is 0.04634617641568184\n",
      "loss at step  1034 is 0.047038573771715164\n",
      "loss at step  1035 is 0.053548961877822876\n",
      "loss at step  1036 is 0.046245601028203964\n",
      "loss at step  1037 is 0.052533116191625595\n",
      "loss at step  1038 is 0.044183798134326935\n",
      "loss at step  1039 is 0.04102798178792\n",
      "loss at step  1040 is 0.04592239856719971\n",
      "loss at step  1041 is 0.05555405840277672\n",
      "loss at step  1042 is 0.044908035546541214\n",
      "loss at step  1043 is 0.04520242288708687\n",
      "loss at step  1044 is 0.04170956462621689\n",
      "loss at step  1045 is 0.04161737486720085\n",
      "loss at step  1046 is 0.04553895816206932\n",
      "loss at step  1047 is 0.05954906716942787\n",
      "loss at step  1048 is 0.0404224656522274\n",
      "loss at step  1049 is 0.04620742425322533\n",
      "loss at step  1050 is 0.05021250247955322\n",
      "loss at step  1051 is 0.056303709745407104\n",
      "loss at step  1052 is 0.04481932148337364\n",
      "loss at step  1053 is 0.05128198489546776\n",
      "loss at step  1054 is 0.04018427059054375\n",
      "loss at step  1055 is 0.038171663880348206\n",
      "loss at step  1056 is 0.030441582202911377\n",
      "loss at step  1057 is 0.049631427973508835\n",
      "loss at step  1058 is 0.04256734997034073\n",
      "loss at step  1059 is 0.050651904195547104\n",
      "loss at step  1060 is 0.03017476014792919\n",
      "loss at step  1061 is 0.04747800901532173\n",
      "loss at step  1062 is 0.057503484189510345\n",
      "loss at step  1063 is 0.040273237973451614\n",
      "loss at step  1064 is 0.052953802049160004\n",
      "loss at step  1065 is 0.04705129563808441\n",
      "loss at step  1066 is 0.047616660594940186\n",
      "loss at step  1067 is 0.050970710813999176\n",
      "loss at step  1068 is 0.04881136864423752\n",
      "loss at step  1069 is 0.04226629436016083\n",
      "loss at step  1070 is 0.039582982659339905\n",
      "loss at step  1071 is 0.041593339294195175\n",
      "loss at step  1072 is 0.05508214607834816\n",
      "loss at step  1073 is 0.04471226781606674\n",
      "loss at step  1074 is 0.0319671593606472\n",
      "loss at step  1075 is 0.05947113409638405\n",
      "loss at step  1076 is 0.04474424943327904\n",
      "loss at step  1077 is 0.04935995861887932\n",
      "loss at step  1078 is 0.04663512483239174\n",
      "loss at step  1079 is 0.046479444950819016\n",
      "loss at step  1080 is 0.05833873152732849\n",
      "loss at step  1081 is 0.04548891633749008\n",
      "loss at step  1082 is 0.04886548966169357\n",
      "loss at step  1083 is 0.03352092206478119\n",
      "loss at step  1084 is 0.0428941510617733\n",
      "loss at step  1085 is 0.046123962849378586\n",
      "loss at step  1086 is 0.04986068978905678\n",
      "loss at step  1087 is 0.03833114728331566\n",
      "loss at step  1088 is 0.04715140536427498\n",
      "loss at step  1089 is 0.0393197126686573\n",
      "loss at step  1090 is 0.04947594180703163\n",
      "loss at step  1091 is 0.03614203631877899\n",
      "loss at step  1092 is 0.0593755766749382\n",
      "loss at step  1093 is 0.04751749336719513\n",
      "loss at step  1094 is 0.04835420101881027\n",
      "loss at step  1095 is 0.050480280071496964\n",
      "loss at step  1096 is 0.04698524251580238\n",
      "loss at step  1097 is 0.06142205744981766\n",
      "loss at step  1098 is 0.03696848079562187\n",
      "loss at step  1099 is 0.05071926489472389\n",
      "loss at step  1100 is 0.06801781058311462\n",
      "loss at step  1101 is 0.038169991225004196\n",
      "loss at step  1102 is 0.05246671661734581\n",
      "loss at step  1103 is 0.04835346341133118\n",
      "loss at step  1104 is 0.05378491431474686\n",
      "loss at step  1105 is 0.0410463772714138\n",
      "loss at step  1106 is 0.043435756117105484\n",
      "loss at step  1107 is 0.047850050032138824\n",
      "loss at step  1108 is 0.03461487218737602\n",
      "loss at step  1109 is 0.04927706718444824\n",
      "loss at step  1110 is 0.041450127959251404\n",
      "loss at step  1111 is 0.03966140002012253\n",
      "loss at step  1112 is 0.039087481796741486\n",
      "loss at step  1113 is 0.03967628628015518\n",
      "loss at step  1114 is 0.04865116626024246\n",
      "loss at step  1115 is 0.04653576761484146\n",
      "loss at step  1116 is 0.045505672693252563\n",
      "loss at step  1117 is 0.05098644271492958\n",
      "loss at step  1118 is 0.04758968576788902\n",
      "loss at step  1119 is 0.04224551469087601\n",
      "loss at step  1120 is 0.044955767691135406\n",
      "loss at step  1121 is 0.05266130343079567\n",
      "loss at step  1122 is 0.044471971690654755\n",
      "loss at step  1123 is 0.03877831995487213\n",
      "loss at step  1124 is 0.05022535100579262\n",
      "loss at step  1125 is 0.0411190465092659\n",
      "loss at step  1126 is 0.03802986815571785\n",
      "loss at step  1127 is 0.05116855353116989\n",
      "loss at step  1128 is 0.04018668457865715\n",
      "loss at step  1129 is 0.04608399420976639\n",
      "loss at step  1130 is 0.046250153332948685\n",
      "loss at step  1131 is 0.05009588599205017\n",
      "loss at step  1132 is 0.0459061898291111\n",
      "loss at step  1133 is 0.04639751464128494\n",
      "loss at step  1134 is 0.041888974606990814\n",
      "loss at step  1135 is 0.057476893067359924\n",
      "loss at step  1136 is 0.0380675345659256\n",
      "loss at step  1137 is 0.05296184867620468\n",
      "loss at step  1138 is 0.05015931278467178\n",
      "loss at step  1139 is 0.03664368391036987\n",
      "loss at step  1140 is 0.04489981010556221\n",
      "loss at step  1141 is 0.046116333454847336\n",
      "loss at step  1142 is 0.05006425082683563\n",
      "loss at step  1143 is 0.0429694764316082\n",
      "loss at step  1144 is 0.045021768659353256\n",
      "loss at step  1145 is 0.046605583280324936\n",
      "loss at step  1146 is 0.033639512956142426\n",
      "loss at step  1147 is 0.05021191015839577\n",
      "loss at step  1148 is 0.050197213888168335\n",
      "loss at step  1149 is 0.05117388069629669\n",
      "loss at step  1150 is 0.0480244979262352\n",
      "loss at step  1151 is 0.04875969514250755\n",
      "loss at step  1152 is 0.044037990272045135\n",
      "loss at step  1153 is 0.04718121886253357\n",
      "loss at step  1154 is 0.05275550112128258\n",
      "loss at step  1155 is 0.03355875611305237\n",
      "loss at step  1156 is 0.023147013038396835\n",
      "loss at step  1157 is 0.03255382552742958\n",
      "loss at step  1158 is 0.03129160776734352\n",
      "loss at step  1159 is 0.041812412440776825\n",
      "loss at step  1160 is 0.050933968275785446\n",
      "loss at step  1161 is 0.04815719649195671\n",
      "loss at step  1162 is 0.050604693591594696\n",
      "loss at step  1163 is 0.04023737087845802\n",
      "loss at step  1164 is 0.051852427423000336\n",
      "loss at step  1165 is 0.04422786459326744\n",
      "loss at step  1166 is 0.049061454832553864\n",
      "loss at step  1167 is 0.038140054792165756\n",
      "loss at step  1168 is 0.04558693990111351\n",
      "loss at step  1169 is 0.0436258465051651\n",
      "loss at step  1170 is 0.05824372172355652\n",
      "loss at step  1171 is 0.038559187203645706\n",
      "loss at step  1172 is 0.043580252677202225\n",
      "loss at step  1173 is 0.04763156548142433\n",
      "loss at step  1174 is 0.040430083870887756\n",
      "loss at step  1175 is 0.04091304540634155\n",
      "loss at step  1176 is 0.046152278780937195\n",
      "loss at step  1177 is 0.04316720739006996\n",
      "loss at step  1178 is 0.03049158863723278\n",
      "loss at step  1179 is 0.033207882195711136\n",
      "loss at step  1180 is 0.05980378016829491\n",
      "loss at step  1181 is 0.03700891137123108\n",
      "loss at step  1182 is 0.045180413872003555\n",
      "loss at step  1183 is 0.04037047550082207\n",
      "loss at step  1184 is 0.03738325461745262\n",
      "loss at step  1185 is 0.04173259064555168\n",
      "loss at step  1186 is 0.043171562254428864\n",
      "loss at step  1187 is 0.05293555185198784\n",
      "loss at step  1188 is 0.033245161175727844\n",
      "loss at step  1189 is 0.03660333529114723\n",
      "loss at step  1190 is 0.05181077867746353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  1191 is 0.047684572637081146\n",
      "loss at step  1192 is 0.04299648851156235\n",
      "loss at step  1193 is 0.039015479385852814\n",
      "loss at step  1194 is 0.033576373010873795\n",
      "loss at step  1195 is 0.03345489874482155\n",
      "loss at step  1196 is 0.03840284422039986\n",
      "loss at step  1197 is 0.04689905047416687\n",
      "loss at step  1198 is 0.04500482603907585\n",
      "loss at step  1199 is 0.03474261611700058\n",
      "loss at step  1200 is 0.03868996724486351\n",
      "================================  Model Saved  ================================\n",
      "loss at step  1201 is 0.030370168387889862\n",
      "loss at step  1202 is 0.042273107916116714\n",
      "loss at step  1203 is 0.04371834173798561\n",
      "loss at step  1204 is 0.04721643030643463\n",
      "loss at step  1205 is 0.04417109861969948\n",
      "loss at step  1206 is 0.0516110397875309\n",
      "loss at step  1207 is 0.043276384472846985\n",
      "loss at step  1208 is 0.05104317516088486\n",
      "loss at step  1209 is 0.03353770822286606\n",
      "loss at step  1210 is 0.036999572068452835\n",
      "loss at step  1211 is 0.04062625393271446\n",
      "loss at step  1212 is 0.044750578701496124\n",
      "loss at step  1213 is 0.05069911852478981\n",
      "loss at step  1214 is 0.03075466863811016\n",
      "loss at step  1215 is 0.04915232211351395\n",
      "loss at step  1216 is 0.04206910729408264\n",
      "loss at step  1217 is 0.03781795874238014\n",
      "loss at step  1218 is 0.03544638305902481\n",
      "loss at step  1219 is 0.044001590460538864\n",
      "loss at step  1220 is 0.04518911987543106\n",
      "loss at step  1221 is 0.04893310368061066\n",
      "loss at step  1222 is 0.04940728843212128\n",
      "loss at step  1223 is 0.051551152020692825\n",
      "loss at step  1224 is 0.04278605431318283\n",
      "loss at step  1225 is 0.03660636395215988\n",
      "loss at step  1226 is 0.05384586378931999\n",
      "loss at step  1227 is 0.05131230130791664\n",
      "loss at step  1228 is 0.04343852028250694\n",
      "loss at step  1229 is 0.04675382003188133\n",
      "loss at step  1230 is 0.0399969257414341\n",
      "loss at step  1231 is 0.042114246636629105\n",
      "loss at step  1232 is 0.04636020585894585\n",
      "loss at step  1233 is 0.05488630011677742\n",
      "loss at step  1234 is 0.026736101135611534\n",
      "loss at step  1235 is 0.05058376118540764\n",
      "loss at step  1236 is 0.05326136574149132\n",
      "loss at step  1237 is 0.04956406354904175\n",
      "loss at step  1238 is 0.043957918882369995\n",
      "loss at step  1239 is 0.04341842606663704\n",
      "loss at step  1240 is 0.037648871541023254\n",
      "loss at step  1241 is 0.0395875908434391\n",
      "loss at step  1242 is 0.041232235729694366\n",
      "loss at step  1243 is 0.03766147792339325\n",
      "loss at step  1244 is 0.05803695693612099\n",
      "loss at step  1245 is 0.04338535666465759\n",
      "loss at step  1246 is 0.049012839794158936\n",
      "loss at step  1247 is 0.06072428822517395\n",
      "loss at step  1248 is 0.04588709771633148\n",
      "loss at step  1249 is 0.044649720191955566\n",
      "loss at step  1250 is 0.05364638566970825\n",
      "loss at step  1251 is 0.041305288672447205\n",
      "loss at step  1252 is 0.06317485868930817\n",
      "loss at step  1253 is 0.036939892917871475\n",
      "loss at step  1254 is 0.04720564931631088\n",
      "loss at step  1255 is 0.03892058506608009\n",
      "loss at step  1256 is 0.046422578394412994\n",
      "loss at step  1257 is 0.05336184427142143\n",
      "loss at step  1258 is 0.044809553772211075\n",
      "loss at step  1259 is 0.04085119813680649\n",
      "loss at step  1260 is 0.04524201527237892\n",
      "loss at step  1261 is 0.04156707599759102\n",
      "loss at step  1262 is 0.03622313588857651\n",
      "loss at step  1263 is 0.04245619848370552\n",
      "loss at step  1264 is 0.034853048622608185\n",
      "loss at step  1265 is 0.04028771072626114\n",
      "loss at step  1266 is 0.036616381257772446\n",
      "loss at step  1267 is 0.050091058015823364\n",
      "loss at step  1268 is 0.03765882924199104\n",
      "loss at step  1269 is 0.04351543262600899\n",
      "loss at step  1270 is 0.052588362246751785\n",
      "loss at step  1271 is 0.04395347088575363\n",
      "loss at step  1272 is 0.0501348078250885\n",
      "loss at step  1273 is 0.04138492792844772\n",
      "loss at step  1274 is 0.04228659346699715\n",
      "loss at step  1275 is 0.04723067581653595\n",
      "loss at step  1276 is 0.04658930003643036\n",
      "loss at step  1277 is 0.04434378072619438\n",
      "loss at step  1278 is 0.04050662741065025\n",
      "loss at step  1279 is 0.0454297736287117\n",
      "loss at step  1280 is 0.05447818711400032\n",
      "loss at step  1281 is 0.037205711007118225\n",
      "loss at step  1282 is 0.04378803074359894\n",
      "loss at step  1283 is 0.041483327746391296\n",
      "loss at step  1284 is 0.03401486203074455\n",
      "loss at step  1285 is 0.0477992445230484\n",
      "loss at step  1286 is 0.03562713786959648\n",
      "loss at step  1287 is 0.047402650117874146\n",
      "loss at step  1288 is 0.049908559769392014\n",
      "loss at step  1289 is 0.031231537461280823\n",
      "loss at step  1290 is 0.05166062340140343\n",
      "loss at step  1291 is 0.04452763497829437\n",
      "loss at step  1292 is 0.043291520327329636\n",
      "loss at step  1293 is 0.04597589001059532\n",
      "loss at step  1294 is 0.04250310733914375\n",
      "loss at step  1295 is 0.03592770919203758\n",
      "loss at step  1296 is 0.04660506546497345\n",
      "loss at step  1297 is 0.04134972766041756\n",
      "loss at step  1298 is 0.06085630878806114\n",
      "loss at step  1299 is 0.045820172876119614\n",
      "loss at step  1300 is 0.03270648792386055\n",
      "loss at step  1301 is 0.03834434226155281\n",
      "loss at step  1302 is 0.04630763828754425\n",
      "loss at step  1303 is 0.03900637850165367\n",
      "loss at step  1304 is 0.04996192827820778\n",
      "loss at step  1305 is 0.045605599880218506\n",
      "loss at step  1306 is 0.043904468417167664\n",
      "loss at step  1307 is 0.04948369413614273\n",
      "loss at step  1308 is 0.03420088440179825\n",
      "loss at step  1309 is 0.021579883992671967\n",
      "loss at step  1310 is 0.0417264886200428\n",
      "loss at step  1311 is 0.035445619374513626\n",
      "loss at step  1312 is 0.04812217131257057\n",
      "loss at step  1313 is 0.05578658729791641\n",
      "loss at step  1314 is 0.05696752294898033\n",
      "loss at step  1315 is 0.05292034521698952\n",
      "loss at step  1316 is 0.031889282166957855\n",
      "loss at step  1317 is 0.04948396980762482\n",
      "loss at step  1318 is 0.049415040761232376\n",
      "loss at step  1319 is 0.04135292023420334\n",
      "loss at step  1320 is 0.04725322127342224\n",
      "loss at step  1321 is 0.047053590416908264\n",
      "loss at step  1322 is 0.043934863060712814\n",
      "loss at step  1323 is 0.047022026032209396\n",
      "loss at step  1324 is 0.04891781881451607\n",
      "loss at step  1325 is 0.047256410121917725\n",
      "loss at step  1326 is 0.03164181858301163\n",
      "loss at step  1327 is 0.03800682723522186\n",
      "loss at step  1328 is 0.04361853748559952\n",
      "loss at step  1329 is 0.04515073075890541\n",
      "loss at step  1330 is 0.04143382981419563\n",
      "loss at step  1331 is 0.0513540655374527\n",
      "loss at step  1332 is 0.048475589603185654\n",
      "loss at step  1333 is 0.04064151272177696\n",
      "loss at step  1334 is 0.04964761063456535\n",
      "loss at step  1335 is 0.057546719908714294\n",
      "loss at step  1336 is 0.04679054021835327\n",
      "loss at step  1337 is 0.035135310143232346\n",
      "loss at step  1338 is 0.035494375973939896\n",
      "loss at step  1339 is 0.04182080179452896\n",
      "loss at step  1340 is 0.05875755101442337\n",
      "loss at step  1341 is 0.03789494186639786\n",
      "loss at step  1342 is 0.05531292036175728\n",
      "loss at step  1343 is 0.03971460461616516\n",
      "loss at step  1344 is 0.05611224099993706\n",
      "loss at step  1345 is 0.04617816209793091\n",
      "loss at step  1346 is 0.03801492229104042\n",
      "loss at step  1347 is 0.03599782660603523\n",
      "loss at step  1348 is 0.03842826560139656\n",
      "loss at step  1349 is 0.03900555521249771\n",
      "loss at step  1350 is 0.045467887073755264\n",
      "loss at step  1351 is 0.05682075396180153\n",
      "loss at step  1352 is 0.04649000242352486\n",
      "loss at step  1353 is 0.04978838190436363\n",
      "loss at step  1354 is 0.044700976461172104\n",
      "loss at step  1355 is 0.05350572615861893\n",
      "loss at step  1356 is 0.04105053469538689\n",
      "loss at step  1357 is 0.05238397791981697\n",
      "loss at step  1358 is 0.04585091397166252\n",
      "loss at step  1359 is 0.04862045496702194\n",
      "loss at step  1360 is 0.05175057053565979\n",
      "loss at step  1361 is 0.03454595431685448\n",
      "loss at step  1362 is 0.05056850239634514\n",
      "loss at step  1363 is 0.045984186232089996\n",
      "loss at step  1364 is 0.04748739302158356\n",
      "loss at step  1365 is 0.050142042338848114\n",
      "loss at step  1366 is 0.04107066988945007\n",
      "loss at step  1367 is 0.0339684784412384\n",
      "loss at step  1368 is 0.037149179726839066\n",
      "loss at step  1369 is 0.04375070706009865\n",
      "loss at step  1370 is 0.04779869318008423\n",
      "loss at step  1371 is 0.04937651753425598\n",
      "loss at step  1372 is 0.034824009984731674\n",
      "loss at step  1373 is 0.03573228418827057\n",
      "loss at step  1374 is 0.03853263333439827\n",
      "loss at step  1375 is 0.04731999710202217\n",
      "loss at step  1376 is 0.05128474906086922\n",
      "loss at step  1377 is 0.050576746463775635\n",
      "loss at step  1378 is 0.05644051358103752\n",
      "loss at step  1379 is 0.0431322306394577\n",
      "loss at step  1380 is 0.04067278653383255\n",
      "loss at step  1381 is 0.04268897324800491\n",
      "loss at step  1382 is 0.04398548603057861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  1383 is 0.04638289660215378\n",
      "loss at step  1384 is 0.05171908810734749\n",
      "loss at step  1385 is 0.04847464710474014\n",
      "loss at step  1386 is 0.04676055163145065\n",
      "loss at step  1387 is 0.04543260857462883\n",
      "loss at step  1388 is 0.051447395235300064\n",
      "loss at step  1389 is 0.05356360971927643\n",
      "loss at step  1390 is 0.04443008080124855\n",
      "loss at step  1391 is 0.03717942535877228\n",
      "loss at step  1392 is 0.05052400380373001\n",
      "loss at step  1393 is 0.033467575907707214\n",
      "loss at step  1394 is 0.03681168332695961\n",
      "loss at step  1395 is 0.03575574606657028\n",
      "loss at step  1396 is 0.04490463435649872\n",
      "loss at step  1397 is 0.047654591500759125\n",
      "loss at step  1398 is 0.04160338267683983\n",
      "loss at step  1399 is 0.05367637425661087\n",
      "loss at step  1400 is 0.029620494693517685\n",
      "================================  Model Saved  ================================\n",
      "loss at step  1401 is 0.04151009023189545\n",
      "loss at step  1402 is 0.0397162064909935\n",
      "loss at step  1403 is 0.05211733281612396\n",
      "loss at step  1404 is 0.03195254132151604\n",
      "loss at step  1405 is 0.03872630000114441\n",
      "loss at step  1406 is 0.05010185018181801\n",
      "loss at step  1407 is 0.03997751697897911\n",
      "loss at step  1408 is 0.05754983797669411\n",
      "loss at step  1409 is 0.0379481203854084\n",
      "loss at step  1410 is 0.03849071264266968\n",
      "loss at step  1411 is 0.0454438179731369\n",
      "loss at step  1412 is 0.029954461380839348\n",
      "loss at step  1413 is 0.04893394187092781\n",
      "loss at step  1414 is 0.04940570890903473\n",
      "loss at step  1415 is 0.037997301667928696\n",
      "loss at step  1416 is 0.051990751177072525\n",
      "loss at step  1417 is 0.05327669531106949\n",
      "loss at step  1418 is 0.05023445934057236\n",
      "loss at step  1419 is 0.03962768614292145\n",
      "loss at step  1420 is 0.03688180074095726\n",
      "loss at step  1421 is 0.03421294689178467\n",
      "loss at step  1422 is 0.028486797586083412\n",
      "loss at step  1423 is 0.05169644206762314\n",
      "loss at step  1424 is 0.04544268921017647\n",
      "loss at step  1425 is 0.038680415600538254\n",
      "loss at step  1426 is 0.04119472578167915\n",
      "loss at step  1427 is 0.045070938766002655\n",
      "loss at step  1428 is 0.04352487623691559\n",
      "loss at step  1429 is 0.03503567725419998\n",
      "loss at step  1430 is 0.04549364373087883\n",
      "loss at step  1431 is 0.05470332130789757\n",
      "loss at step  1432 is 0.05424187332391739\n",
      "loss at step  1433 is 0.04930761829018593\n",
      "loss at step  1434 is 0.04989807307720184\n",
      "loss at step  1435 is 0.05225317180156708\n",
      "loss at step  1436 is 0.04922116920351982\n",
      "loss at step  1437 is 0.04097796231508255\n",
      "loss at step  1438 is 0.0348646342754364\n",
      "loss at step  1439 is 0.03747263550758362\n",
      "loss at step  1440 is 0.04655347764492035\n",
      "loss at step  1441 is 0.03984517976641655\n",
      "loss at step  1442 is 0.03998958319425583\n",
      "loss at step  1443 is 0.03961065784096718\n",
      "loss at step  1444 is 0.044489786028862\n",
      "loss at step  1445 is 0.05286721512675285\n",
      "loss at step  1446 is 0.04486308991909027\n",
      "loss at step  1447 is 0.03392898291349411\n",
      "loss at step  1448 is 0.03476541116833687\n",
      "loss at step  1449 is 0.04721461609005928\n",
      "loss at step  1450 is 0.03717498481273651\n",
      "loss at step  1451 is 0.05496818944811821\n",
      "loss at step  1452 is 0.04598063975572586\n",
      "loss at step  1453 is 0.04592857509851456\n",
      "loss at step  1454 is 0.04336757957935333\n",
      "loss at step  1455 is 0.03524220734834671\n",
      "loss at step  1456 is 0.03627202659845352\n",
      "loss at step  1457 is 0.05408090725541115\n",
      "loss at step  1458 is 0.035253606736660004\n",
      "loss at step  1459 is 0.03604791313409805\n",
      "loss at step  1460 is 0.03618161007761955\n",
      "loss at step  1461 is 0.045858174562454224\n",
      "loss at step  1462 is 0.04785819351673126\n",
      "loss at step  1463 is 0.04580148309469223\n",
      "loss at step  1464 is 0.055235687643289566\n",
      "loss at step  1465 is 0.04377212002873421\n",
      "loss at step  1466 is 0.04379476234316826\n",
      "loss at step  1467 is 0.036711160093545914\n",
      "loss at step  1468 is 0.029173776507377625\n",
      "loss at step  1469 is 0.044870685786008835\n",
      "loss at step  1470 is 0.03251887485384941\n",
      "loss at step  1471 is 0.03721015900373459\n",
      "loss at step  1472 is 0.03576109930872917\n",
      "loss at step  1473 is 0.04548896104097366\n",
      "loss at step  1474 is 0.04526940733194351\n",
      "loss at step  1475 is 0.03786114230751991\n",
      "loss at step  1476 is 0.019105546176433563\n",
      "loss at step  1477 is 0.04910484328866005\n",
      "loss at step  1478 is 0.04814310744404793\n",
      "loss at step  1479 is 0.040890924632549286\n",
      "loss at step  1480 is 0.034573186188936234\n",
      "loss at step  1481 is 0.02778414450585842\n",
      "loss at step  1482 is 0.035532157868146896\n",
      "loss at step  1483 is 0.04321949928998947\n",
      "loss at step  1484 is 0.039351947605609894\n",
      "loss at step  1485 is 0.04574419930577278\n",
      "loss at step  1486 is 0.04367050901055336\n",
      "loss at step  1487 is 0.04008755832910538\n",
      "loss at step  1488 is 0.04035291448235512\n",
      "loss at step  1489 is 0.047960877418518066\n",
      "loss at step  1490 is 0.0357617549598217\n",
      "loss at step  1491 is 0.01900540664792061\n",
      "loss at step  1492 is 0.019061505794525146\n",
      "loss at step  1493 is 0.0462375283241272\n",
      "loss at step  1494 is 0.039283622056245804\n",
      "loss at step  1495 is 0.043229810893535614\n",
      "loss at step  1496 is 0.04944920167326927\n",
      "loss at step  1497 is 0.0395941436290741\n",
      "loss at step  1498 is 0.049138911068439484\n",
      "loss at step  1499 is 0.05637996271252632\n",
      "loss at step  1500 is 0.03136305883526802\n",
      "loss at step  1501 is 0.033864907920360565\n",
      "loss at step  1502 is 0.044796597212553024\n",
      "loss at step  1503 is 0.04673337563872337\n",
      "loss at step  1504 is 0.04333871603012085\n",
      "loss at step  1505 is 0.04691513627767563\n",
      "loss at step  1506 is 0.05569153651595116\n",
      "loss at step  1507 is 0.03751915693283081\n",
      "loss at step  1508 is 0.048854656517505646\n",
      "loss at step  1509 is 0.046677619218826294\n",
      "loss at step  1510 is 0.04783442243933678\n",
      "loss at step  1511 is 0.05224715918302536\n",
      "loss at step  1512 is 0.04531179741024971\n",
      "loss at step  1513 is 0.04175492376089096\n",
      "loss at step  1514 is 0.04255614057183266\n",
      "loss at step  1515 is 0.04077914357185364\n",
      "loss at step  1516 is 0.034930270165205\n",
      "loss at step  1517 is 0.04282485693693161\n",
      "loss at step  1518 is 0.03736041858792305\n",
      "loss at step  1519 is 0.0449073426425457\n",
      "loss at step  1520 is 0.05331313610076904\n",
      "loss at step  1521 is 0.0292894896119833\n",
      "loss at step  1522 is 0.029049301519989967\n",
      "loss at step  1523 is 0.03866687789559364\n",
      "loss at step  1524 is 0.04803331196308136\n",
      "loss at step  1525 is 0.035704538226127625\n",
      "loss at step  1526 is 0.052055519074201584\n",
      "loss at step  1527 is 0.042484648525714874\n",
      "loss at step  1528 is 0.03029835969209671\n",
      "loss at step  1529 is 0.04907555505633354\n",
      "loss at step  1530 is 0.029895255342125893\n",
      "loss at step  1531 is 0.04303167760372162\n",
      "loss at step  1532 is 0.039325419813394547\n",
      "loss at step  1533 is 0.04300087317824364\n",
      "loss at step  1534 is 0.050644174218177795\n",
      "loss at step  1535 is 0.038417305797338486\n",
      "loss at step  1536 is 0.047909900546073914\n",
      "loss at step  1537 is 0.03993246331810951\n",
      "loss at step  1538 is 0.045889560133218765\n",
      "loss at step  1539 is 0.04731449484825134\n",
      "loss at step  1540 is 0.03702355921268463\n",
      "loss at step  1541 is 0.04365350306034088\n",
      "loss at step  1542 is 0.029031159356236458\n",
      "loss at step  1543 is 0.05143904685974121\n",
      "loss at step  1544 is 0.03987425938248634\n",
      "loss at step  1545 is 0.04554934427142143\n",
      "loss at step  1546 is 0.0401139035820961\n",
      "loss at step  1547 is 0.040597103536129\n",
      "loss at step  1548 is 0.041216351091861725\n",
      "loss at step  1549 is 0.05555657669901848\n",
      "loss at step  1550 is 0.04348444566130638\n",
      "loss at step  1551 is 0.05214347690343857\n",
      "loss at step  1552 is 0.04352017492055893\n",
      "loss at step  1553 is 0.044175490736961365\n",
      "loss at step  1554 is 0.05247360095381737\n",
      "loss at step  1555 is 0.05068434402346611\n",
      "loss at step  1556 is 0.04373843967914581\n",
      "loss at step  1557 is 0.054064515978097916\n",
      "loss at step  1558 is 0.05668840929865837\n",
      "loss at step  1559 is 0.040697261691093445\n",
      "loss at step  1560 is 0.03451919183135033\n",
      "loss at step  1561 is 0.05176315829157829\n",
      "loss at step  1562 is 0.050512589514255524\n",
      "loss at step  1563 is 0.0314587727189064\n",
      "loss at step  1564 is 0.041907671838998795\n",
      "loss at step  1565 is 0.052316222339868546\n",
      "loss at step  1566 is 0.04015126824378967\n",
      "loss at step  1567 is 0.045640457421541214\n",
      "loss at step  1568 is 0.038494773209095\n",
      "loss at step  1569 is 0.038030195981264114\n",
      "loss at step  1570 is 0.030266540125012398\n",
      "loss at step  1571 is 0.03727726638317108\n",
      "loss at step  1572 is 0.052592046558856964\n",
      "loss at step  1573 is 0.01685638353228569\n",
      "loss at step  1574 is 0.04944309964776039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  1575 is 0.04900915548205376\n",
      "loss at step  1576 is 0.06433957070112228\n",
      "loss at step  1577 is 0.041744545102119446\n",
      "loss at step  1578 is 0.03738892823457718\n",
      "loss at step  1579 is 0.05592961981892586\n",
      "loss at step  1580 is 0.0392436683177948\n",
      "loss at step  1581 is 0.04062354937195778\n",
      "loss at step  1582 is 0.05849076807498932\n",
      "loss at step  1583 is 0.034357666969299316\n",
      "loss at step  1584 is 0.041222844272851944\n",
      "loss at step  1585 is 0.04265617951750755\n",
      "loss at step  1586 is 0.03664451837539673\n",
      "loss at step  1587 is 0.04260089620947838\n",
      "loss at step  1588 is 0.047414299100637436\n",
      "loss at step  1589 is 0.0327291265130043\n",
      "loss at step  1590 is 0.050153013318777084\n",
      "loss at step  1591 is 0.03186272084712982\n",
      "loss at step  1592 is 0.0379069559276104\n",
      "loss at step  1593 is 0.05115680769085884\n",
      "loss at step  1594 is 0.03925231844186783\n",
      "loss at step  1595 is 0.046080850064754486\n",
      "loss at step  1596 is 0.04256683215498924\n",
      "loss at step  1597 is 0.04261303320527077\n",
      "loss at step  1598 is 0.04135425016283989\n",
      "loss at step  1599 is 0.03199745714664459\n",
      "loss at step  1600 is 0.04044753685593605\n",
      "================================  Model Saved  ================================\n",
      "loss at step  1601 is 0.038078948855400085\n",
      "loss at step  1602 is 0.03887847065925598\n",
      "loss at step  1603 is 0.04243123158812523\n",
      "loss at step  1604 is 0.04742302745580673\n",
      "loss at step  1605 is 0.05962758511304855\n",
      "loss at step  1606 is 0.049638379365205765\n",
      "loss at step  1607 is 0.04673952981829643\n",
      "loss at step  1608 is 0.03821876645088196\n",
      "loss at step  1609 is 0.0391237698495388\n",
      "loss at step  1610 is 0.04226323962211609\n",
      "loss at step  1611 is 0.03755103424191475\n",
      "loss at step  1612 is 0.0477125309407711\n",
      "loss at step  1613 is 0.04701446741819382\n",
      "loss at step  1614 is 0.03261620178818703\n",
      "loss at step  1615 is 0.041290782392024994\n",
      "loss at step  1616 is 0.0446675568819046\n",
      "loss at step  1617 is 0.045017633587121964\n",
      "loss at step  1618 is 0.05220881104469299\n",
      "loss at step  1619 is 0.04587803781032562\n",
      "loss at step  1620 is 0.04232333227992058\n",
      "loss at step  1621 is 0.050111912190914154\n",
      "loss at step  1622 is 0.0530000738799572\n",
      "loss at step  1623 is 0.04822425916790962\n",
      "loss at step  1624 is 0.04503822699189186\n",
      "loss at step  1625 is 0.031365785747766495\n",
      "loss at step  1626 is 0.03888903185725212\n",
      "loss at step  1627 is 0.041844308376312256\n",
      "loss at step  1628 is 0.046648021787405014\n",
      "loss at step  1629 is 0.04492007941007614\n",
      "loss at step  1630 is 0.039774175733327866\n",
      "loss at step  1631 is 0.0377633236348629\n",
      "loss at step  1632 is 0.04194594919681549\n",
      "loss at step  1633 is 0.0447176992893219\n",
      "loss at step  1634 is 0.035862475633621216\n",
      "loss at step  1635 is 0.04233510047197342\n",
      "loss at step  1636 is 0.043957557529211044\n",
      "loss at step  1637 is 0.03570597991347313\n",
      "loss at step  1638 is 0.0293838232755661\n",
      "loss at step  1639 is 0.043424833565950394\n",
      "loss at step  1640 is 0.04039944335818291\n",
      "loss at step  1641 is 0.03302965685725212\n",
      "loss at step  1642 is 0.05317630246281624\n",
      "loss at step  1643 is 0.043067850172519684\n",
      "loss at step  1644 is 0.05174079164862633\n",
      "loss at step  1645 is 0.05215926468372345\n",
      "loss at step  1646 is 0.033031582832336426\n",
      "loss at step  1647 is 0.03852972388267517\n",
      "loss at step  1648 is 0.04653889685869217\n",
      "loss at step  1649 is 0.037712838500738144\n",
      "loss at step  1650 is 0.06829231232404709\n",
      "loss at step  1651 is 0.037269748747348785\n",
      "loss at step  1652 is 0.05930928885936737\n",
      "loss at step  1653 is 0.040927570313215256\n",
      "loss at step  1654 is 0.060096584260463715\n",
      "loss at step  1655 is 0.04202036187052727\n",
      "loss at step  1656 is 0.0451640784740448\n",
      "loss at step  1657 is 0.0360926128923893\n",
      "loss at step  1658 is 0.04447965696454048\n",
      "loss at step  1659 is 0.05339796096086502\n",
      "loss at step  1660 is 0.051235049962997437\n",
      "loss at step  1661 is 0.045776814222335815\n",
      "loss at step  1662 is 0.04525885730981827\n",
      "loss at step  1663 is 0.03023729845881462\n",
      "loss at step  1664 is 0.04096360504627228\n",
      "loss at step  1665 is 0.03630579262971878\n",
      "loss at step  1666 is 0.03682762011885643\n",
      "loss at step  1667 is 0.03679436445236206\n",
      "loss at step  1668 is 0.039486996829509735\n",
      "loss at step  1669 is 0.04937367886304855\n",
      "loss at step  1670 is 0.042726390063762665\n",
      "loss at step  1671 is 0.03963012993335724\n",
      "loss at step  1672 is 0.03823525458574295\n",
      "loss at step  1673 is 0.040762320160865784\n",
      "loss at step  1674 is 0.032829638570547104\n",
      "loss at step  1675 is 0.04582621529698372\n",
      "loss at step  1676 is 0.040543582290410995\n",
      "loss at step  1677 is 0.042295582592487335\n",
      "loss at step  1678 is 0.051992904394865036\n",
      "loss at step  1679 is 0.05228403955698013\n",
      "loss at step  1680 is 0.039248209446668625\n",
      "loss at step  1681 is 0.041762035340070724\n",
      "loss at step  1682 is 0.04302062466740608\n",
      "loss at step  1683 is 0.0310785211622715\n",
      "loss at step  1684 is 0.05601859837770462\n",
      "loss at step  1685 is 0.04750073328614235\n",
      "loss at step  1686 is 0.05268511176109314\n",
      "loss at step  1687 is 0.043415747582912445\n",
      "loss at step  1688 is 0.04845324903726578\n",
      "loss at step  1689 is 0.04245287552475929\n",
      "loss at step  1690 is 0.053713370114564896\n",
      "loss at step  1691 is 0.04598093405365944\n",
      "loss at step  1692 is 0.04266056790947914\n",
      "loss at step  1693 is 0.03966672718524933\n",
      "loss at step  1694 is 0.04770888760685921\n",
      "loss at step  1695 is 0.048885706812143326\n",
      "loss at step  1696 is 0.053595371544361115\n",
      "loss at step  1697 is 0.04149480536580086\n",
      "loss at step  1698 is 0.043359458446502686\n",
      "loss at step  1699 is 0.04243592545390129\n",
      "loss at step  1700 is 0.04370732232928276\n",
      "loss at step  1701 is 0.04250470548868179\n",
      "loss at step  1702 is 0.05542077124118805\n",
      "loss at step  1703 is 0.03185972198843956\n",
      "loss at step  1704 is 0.05001538619399071\n",
      "loss at step  1705 is 0.05183245986700058\n",
      "loss at step  1706 is 0.044807884842157364\n",
      "loss at step  1707 is 0.04237467795610428\n",
      "loss at step  1708 is 0.04710615053772926\n",
      "loss at step  1709 is 0.040177326649427414\n",
      "loss at step  1710 is 0.03983137384057045\n",
      "loss at step  1711 is 0.042951274663209915\n",
      "loss at step  1712 is 0.03994631767272949\n",
      "loss at step  1713 is 0.047813139855861664\n",
      "loss at step  1714 is 0.037290725857019424\n",
      "loss at step  1715 is 0.04564429819583893\n",
      "loss at step  1716 is 0.03910120949149132\n",
      "loss at step  1717 is 0.04016350209712982\n",
      "loss at step  1718 is 0.04405955970287323\n",
      "loss at step  1719 is 0.04175815358757973\n",
      "loss at step  1720 is 0.033730506896972656\n",
      "loss at step  1721 is 0.0498395636677742\n",
      "loss at step  1722 is 0.04553965851664543\n",
      "loss at step  1723 is 0.04537206515669823\n",
      "loss at step  1724 is 0.03757718205451965\n",
      "loss at step  1725 is 0.04919160157442093\n",
      "loss at step  1726 is 0.03989025950431824\n",
      "loss at step  1727 is 0.03326491266489029\n",
      "loss at step  1728 is 0.045548126101493835\n",
      "loss at step  1729 is 0.05770120769739151\n",
      "loss at step  1730 is 0.03583748638629913\n",
      "loss at step  1731 is 0.03918201103806496\n",
      "loss at step  1732 is 0.02854125015437603\n",
      "loss at step  1733 is 0.03779274970293045\n",
      "loss at step  1734 is 0.05576210841536522\n",
      "loss at step  1735 is 0.04037347435951233\n",
      "loss at step  1736 is 0.03714476898312569\n",
      "loss at step  1737 is 0.043689511716365814\n",
      "loss at step  1738 is 0.05113555118441582\n",
      "loss at step  1739 is 0.04994513466954231\n",
      "loss at step  1740 is 0.05098133534193039\n",
      "loss at step  1741 is 0.04470323398709297\n",
      "loss at step  1742 is 0.04743243008852005\n",
      "loss at step  1743 is 0.04313124343752861\n",
      "loss at step  1744 is 0.03264393284916878\n",
      "loss at step  1745 is 0.04894464090466499\n",
      "loss at step  1746 is 0.03361975774168968\n",
      "loss at step  1747 is 0.03940174728631973\n",
      "loss at step  1748 is 0.039152149111032486\n",
      "loss at step  1749 is 0.041442159563302994\n",
      "loss at step  1750 is 0.04372274503111839\n",
      "loss at step  1751 is 0.04701153561472893\n",
      "loss at step  1752 is 0.03947371244430542\n",
      "loss at step  1753 is 0.041580114513635635\n",
      "loss at step  1754 is 0.03683418408036232\n",
      "loss at step  1755 is 0.03098871558904648\n",
      "loss at step  1756 is 0.04864925146102905\n",
      "loss at step  1757 is 0.05002779886126518\n",
      "loss at step  1758 is 0.031453199684619904\n",
      "loss at step  1759 is 0.04255453497171402\n",
      "loss at step  1760 is 0.04552637040615082\n",
      "loss at step  1761 is 0.04506469890475273\n",
      "loss at step  1762 is 0.03412080556154251\n",
      "loss at step  1763 is 0.036970965564250946\n",
      "loss at step  1764 is 0.035397935658693314\n",
      "loss at step  1765 is 0.038286127150058746\n",
      "loss at step  1766 is 0.0323396697640419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  1767 is 0.056806404143571854\n",
      "loss at step  1768 is 0.03995731845498085\n",
      "loss at step  1769 is 0.04695530980825424\n",
      "loss at step  1770 is 0.040680959820747375\n",
      "loss at step  1771 is 0.048600658774375916\n",
      "loss at step  1772 is 0.046994730830192566\n",
      "loss at step  1773 is 0.03438325598835945\n",
      "loss at step  1774 is 0.0392194464802742\n",
      "loss at step  1775 is 0.036006540060043335\n",
      "loss at step  1776 is 0.046612806618213654\n",
      "loss at step  1777 is 0.03743681684136391\n",
      "loss at step  1778 is 0.04243630915880203\n",
      "loss at step  1779 is 0.04405377805233002\n",
      "loss at step  1780 is 0.050116684287786484\n",
      "loss at step  1781 is 0.0371754951775074\n",
      "loss at step  1782 is 0.051873937249183655\n",
      "loss at step  1783 is 0.03923952579498291\n",
      "loss at step  1784 is 0.03902656212449074\n",
      "loss at step  1785 is 0.042500875890254974\n",
      "loss at step  1786 is 0.04995395988225937\n",
      "loss at step  1787 is 0.04147876054048538\n",
      "loss at step  1788 is 0.05121228098869324\n",
      "loss at step  1789 is 0.042410410940647125\n",
      "loss at step  1790 is 0.035279955714941025\n",
      "loss at step  1791 is 0.03337908908724785\n",
      "loss at step  1792 is 0.0406830720603466\n",
      "loss at step  1793 is 0.02725922130048275\n",
      "loss at step  1794 is 0.0375165157020092\n",
      "loss at step  1795 is 0.04365779086947441\n",
      "loss at step  1796 is 0.04087621346116066\n",
      "loss at step  1797 is 0.034865934401750565\n",
      "loss at step  1798 is 0.049345869570970535\n",
      "loss at step  1799 is 0.054815929383039474\n",
      "loss at step  1800 is 0.04407589137554169\n",
      "================================  Model Saved  ================================\n",
      "loss at step  1801 is 0.043143656104803085\n",
      "loss at step  1802 is 0.043279021978378296\n",
      "loss at step  1803 is 0.046984996646642685\n",
      "loss at step  1804 is 0.0393030010163784\n",
      "loss at step  1805 is 0.04068800434470177\n",
      "loss at step  1806 is 0.05603097379207611\n",
      "loss at step  1807 is 0.037723369896411896\n",
      "loss at step  1808 is 0.04577864333987236\n",
      "loss at step  1809 is 0.044504694640636444\n",
      "loss at step  1810 is 0.028477758169174194\n",
      "loss at step  1811 is 0.05211665481328964\n",
      "loss at step  1812 is 0.03914223611354828\n",
      "loss at step  1813 is 0.035246603190898895\n",
      "loss at step  1814 is 0.047038305550813675\n",
      "loss at step  1815 is 0.04239622503519058\n",
      "loss at step  1816 is 0.05390813574194908\n",
      "loss at step  1817 is 0.029745815321803093\n",
      "loss at step  1818 is 0.0526275560259819\n",
      "loss at step  1819 is 0.03869601711630821\n",
      "loss at step  1820 is 0.04060762748122215\n",
      "loss at step  1821 is 0.037179913371801376\n",
      "loss at step  1822 is 0.03842834383249283\n",
      "loss at step  1823 is 0.03897688165307045\n",
      "loss at step  1824 is 0.03943612426519394\n",
      "loss at step  1825 is 0.043691109865903854\n",
      "loss at step  1826 is 0.03445880860090256\n",
      "loss at step  1827 is 0.03664543107151985\n",
      "loss at step  1828 is 0.037391576915979385\n",
      "loss at step  1829 is 0.04651881381869316\n",
      "loss at step  1830 is 0.03414206951856613\n",
      "loss at step  1831 is 0.03847731277346611\n",
      "loss at step  1832 is 0.03371569141745567\n",
      "loss at step  1833 is 0.045317575335502625\n",
      "loss at step  1834 is 0.035631176084280014\n",
      "loss at step  1835 is 0.040763646364212036\n",
      "loss at step  1836 is 0.057752691209316254\n",
      "loss at step  1837 is 0.039754681289196014\n",
      "loss at step  1838 is 0.0409230962395668\n",
      "loss at step  1839 is 0.04426509886980057\n",
      "loss at step  1840 is 0.016341034322977066\n",
      "loss at step  1841 is 0.04104648903012276\n",
      "loss at step  1842 is 0.04566869139671326\n",
      "loss at step  1843 is 0.04187347739934921\n",
      "loss at step  1844 is 0.040527038276195526\n",
      "loss at step  1845 is 0.036717306822538376\n",
      "loss at step  1846 is 0.05588383227586746\n",
      "loss at step  1847 is 0.0400347001850605\n",
      "loss at step  1848 is 0.041564274579286575\n",
      "loss at step  1849 is 0.04343545436859131\n",
      "loss at step  1850 is 0.0554007925093174\n",
      "loss at step  1851 is 0.04178526625037193\n",
      "loss at step  1852 is 0.04355834051966667\n",
      "loss at step  1853 is 0.04968634620308876\n",
      "loss at step  1854 is 0.049354199320077896\n",
      "loss at step  1855 is 0.04059222340583801\n",
      "loss at step  1856 is 0.044022757560014725\n",
      "loss at step  1857 is 0.031919751316308975\n",
      "loss at step  1858 is 0.03773581609129906\n",
      "loss at step  1859 is 0.03950151056051254\n",
      "loss at step  1860 is 0.05011821165680885\n",
      "loss at step  1861 is 0.04742123931646347\n",
      "loss at step  1862 is 0.03985029086470604\n",
      "loss at step  1863 is 0.035502683371305466\n",
      "loss at step  1864 is 0.05229514464735985\n",
      "loss at step  1865 is 0.04724446311593056\n",
      "loss at step  1866 is 0.04007457196712494\n",
      "loss at step  1867 is 0.05890006944537163\n",
      "loss at step  1868 is 0.04917493462562561\n",
      "loss at step  1869 is 0.038888562470674515\n",
      "loss at step  1870 is 0.040074363350868225\n",
      "loss at step  1871 is 0.047519754618406296\n",
      "loss at step  1872 is 0.05249252915382385\n",
      "loss at step  1873 is 0.04749087244272232\n",
      "loss at step  1874 is 0.047886140644550323\n",
      "loss at step  1875 is 0.04314161464571953\n",
      "loss at step  1876 is 0.03682800754904747\n",
      "loss at step  1877 is 0.03819176182150841\n",
      "loss at step  1878 is 0.03850657120347023\n",
      "loss at step  1879 is 0.039189741015434265\n",
      "loss at step  1880 is 0.06563068926334381\n",
      "loss at step  1881 is 0.04775252938270569\n",
      "loss at step  1882 is 0.04619590565562248\n",
      "loss at step  1883 is 0.04466749355196953\n",
      "loss at step  1884 is 0.04338906705379486\n",
      "loss at step  1885 is 0.0426936149597168\n",
      "loss at step  1886 is 0.04311003535985947\n",
      "loss at step  1887 is 0.04736366122961044\n",
      "loss at step  1888 is 0.05103578045964241\n",
      "loss at step  1889 is 0.040923770517110825\n",
      "loss at step  1890 is 0.03332091122865677\n",
      "loss at step  1891 is 0.04082566127181053\n",
      "loss at step  1892 is 0.048379722982645035\n",
      "loss at step  1893 is 0.03248763084411621\n",
      "loss at step  1894 is 0.03868325799703598\n",
      "loss at step  1895 is 0.052923742681741714\n",
      "loss at step  1896 is 0.03840910643339157\n",
      "loss at step  1897 is 0.05048022419214249\n",
      "loss at step  1898 is 0.045474059879779816\n",
      "loss at step  1899 is 0.043551500886678696\n",
      "loss at step  1900 is 0.05752665549516678\n",
      "loss at step  1901 is 0.040483854711055756\n",
      "loss at step  1902 is 0.04386019706726074\n",
      "loss at step  1903 is 0.04210827499628067\n",
      "loss at step  1904 is 0.03405763581395149\n",
      "loss at step  1905 is 0.04716028273105621\n",
      "loss at step  1906 is 0.032245561480522156\n",
      "loss at step  1907 is 0.05369554087519646\n",
      "loss at step  1908 is 0.0453336201608181\n",
      "loss at step  1909 is 0.037796225398778915\n",
      "loss at step  1910 is 0.044862616807222366\n",
      "loss at step  1911 is 0.05321456864476204\n",
      "loss at step  1912 is 0.05097085237503052\n",
      "loss at step  1913 is 0.030240247026085854\n",
      "loss at step  1914 is 0.04435456544160843\n",
      "loss at step  1915 is 0.043530263006687164\n",
      "loss at step  1916 is 0.028214219957590103\n",
      "loss at step  1917 is 0.0356004536151886\n",
      "loss at step  1918 is 0.049707330763339996\n",
      "loss at step  1919 is 0.03511801362037659\n",
      "loss at step  1920 is 0.04207701236009598\n",
      "loss at step  1921 is 0.05613250657916069\n",
      "loss at step  1922 is 0.05073457956314087\n",
      "loss at step  1923 is 0.042540669441223145\n",
      "loss at step  1924 is 0.03743179142475128\n",
      "loss at step  1925 is 0.04033270850777626\n",
      "loss at step  1926 is 0.04174184799194336\n",
      "loss at step  1927 is 0.028093963861465454\n",
      "loss at step  1928 is 0.02609039470553398\n",
      "loss at step  1929 is 0.05388105288147926\n",
      "loss at step  1930 is 0.040306273847818375\n",
      "loss at step  1931 is 0.04491520673036575\n",
      "loss at step  1932 is 0.038533251732587814\n",
      "loss at step  1933 is 0.04417748376727104\n",
      "loss at step  1934 is 0.022705325856804848\n",
      "loss at step  1935 is 0.036270707845687866\n",
      "loss at step  1936 is 0.05068276822566986\n",
      "loss at step  1937 is 0.05024954676628113\n",
      "loss at step  1938 is 0.03276915103197098\n",
      "loss at step  1939 is 0.04787280037999153\n",
      "loss at step  1940 is 0.048001326620578766\n",
      "loss at step  1941 is 0.05876784399151802\n",
      "loss at step  1942 is 0.038412198424339294\n",
      "loss at step  1943 is 0.048895303159952164\n",
      "loss at step  1944 is 0.04934109374880791\n",
      "loss at step  1945 is 0.05700339376926422\n",
      "loss at step  1946 is 0.038386911153793335\n",
      "loss at step  1947 is 0.04317809268832207\n",
      "loss at step  1948 is 0.0374964103102684\n",
      "loss at step  1949 is 0.031913887709379196\n",
      "loss at step  1950 is 0.040400560945272446\n",
      "loss at step  1951 is 0.05213022604584694\n",
      "loss at step  1952 is 0.03454742208123207\n",
      "loss at step  1953 is 0.0404074527323246\n",
      "loss at step  1954 is 0.03723841905593872\n",
      "loss at step  1955 is 0.0651061162352562\n",
      "loss at step  1956 is 0.040155988186597824\n",
      "loss at step  1957 is 0.03431124985218048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  1958 is 0.05055830255150795\n",
      "loss at step  1959 is 0.05001654103398323\n",
      "loss at step  1960 is 0.03987341746687889\n",
      "loss at step  1961 is 0.041048359125852585\n",
      "loss at step  1962 is 0.05088772624731064\n",
      "loss at step  1963 is 0.03404058516025543\n",
      "loss at step  1964 is 0.03852245584130287\n",
      "loss at step  1965 is 0.041525520384311676\n",
      "loss at step  1966 is 0.04532378539443016\n",
      "loss at step  1967 is 0.043445050716400146\n",
      "loss at step  1968 is 0.04864737391471863\n",
      "loss at step  1969 is 0.04815904423594475\n",
      "loss at step  1970 is 0.037218671292066574\n",
      "loss at step  1971 is 0.0450030080974102\n",
      "loss at step  1972 is 0.05292849242687225\n",
      "loss at step  1973 is 0.032135725021362305\n",
      "loss at step  1974 is 0.03740433603525162\n",
      "loss at step  1975 is 0.03477131575345993\n",
      "loss at step  1976 is 0.042451873421669006\n",
      "loss at step  1977 is 0.03981240466237068\n",
      "loss at step  1978 is 0.04797176644206047\n",
      "loss at step  1979 is 0.024135489016771317\n",
      "loss at step  1980 is 0.04150734841823578\n",
      "loss at step  1981 is 0.05015362426638603\n",
      "loss at step  1982 is 0.036104682832956314\n",
      "loss at step  1983 is 0.033734340220689774\n",
      "loss at step  1984 is 0.04420856758952141\n",
      "loss at step  1985 is 0.02183232083916664\n",
      "loss at step  1986 is 0.024286862462759018\n",
      "loss at step  1987 is 0.028432266786694527\n",
      "loss at step  1988 is 0.043853119015693665\n",
      "loss at step  1989 is 0.03939675912261009\n",
      "loss at step  1990 is 0.04524260386824608\n",
      "loss at step  1991 is 0.03345344588160515\n",
      "loss at step  1992 is 0.047452375292778015\n",
      "loss at step  1993 is 0.031780216842889786\n",
      "loss at step  1994 is 0.05608915910124779\n",
      "loss at step  1995 is 0.018637550994753838\n",
      "loss at step  1996 is 0.03763861581683159\n",
      "loss at step  1997 is 0.03776489943265915\n",
      "loss at step  1998 is 0.05947146564722061\n",
      "loss at step  1999 is 0.038990430533885956\n",
      "loss at step  2000 is 0.045451849699020386\n",
      "================================  Model Saved  ================================\n",
      "loss at step  2001 is 0.04150005057454109\n",
      "loss at step  2002 is 0.04827185347676277\n",
      "loss at step  2003 is 0.03193790093064308\n",
      "loss at step  2004 is 0.039290282875299454\n",
      "loss at step  2005 is 0.041316017508506775\n",
      "loss at step  2006 is 0.04084720090031624\n",
      "loss at step  2007 is 0.048027653247117996\n",
      "loss at step  2008 is 0.034315694123506546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7a8cf92ed57c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAttr_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-0b97174a9d45>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/imaterialist-fashion-2020-fgvc7/train/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMGS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_single_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mmask_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "loss_tracking=0\n",
    "for i in tqdm(range(last, len(Attr_dataset)//batch_size, 1)):\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        I, T = Attr_dataset[i*batch_size+j]\n",
    "        images.append(I)\n",
    "        targets.append(T)\n",
    "        \n",
    "    images = torch.tensor(images, dtype=torch.float32).to(DEVICE)\n",
    "    targets = torch.tensor(targets, dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    out = AttrM(images)\n",
    "    loss =  criterion(out, targets)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('loss at step ', i, 'is', loss.item())\n",
    "    loss_tracking+=loss.item()\n",
    "    \n",
    "    if i%saving_steps==0:\n",
    "        print('================================  Model Saved  ================================')\n",
    "        torch.save(AttrM, 'models/Attr_epoch_'+str(epoch)+'_step_'+str(i)+'_Loss_'+str(loss_tracking/(i+1-last)))\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
